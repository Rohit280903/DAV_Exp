{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyZyp54gOwZgR4/dZXbNG9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohit280903/DAV_Exp/blob/main/DAV_Exp7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Experiment - 7: Perform the steps involved in Text Analytics in Python & R**"
      ],
      "metadata": {
        "id": "BPxe-OwDgj_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab Outcomes (LO):**\n",
        "\n",
        "*   Design Text Analytics Application on a given data set. (LO4)\n",
        "\n"
      ],
      "metadata": {
        "id": "uptzQTRjgqky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Theory :**"
      ],
      "metadata": {
        "id": "zw8x9Zq2hI0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Explore 5 Python Text Analytics Libraries in Python and write their features & applications."
      ],
      "metadata": {
        "id": "0AmeL5gmjNKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have an understanding of what natural language processing can achieve and the purpose of Python NLP libraries, let’s take a look at some of the best options that are currently available.\n",
        "\n",
        "**1. TextBlob**\n",
        "\n",
        "TextBlob is a Python (2 and 3) library that is used to process textual data, with a primary focus on making common text-processing functions accessible via easy-to-use interfaces. Objects within TextBlob can be used as Python strings that can deliver NLP functionality to help build text analysis applications.\n",
        "\n",
        "TextBlob’s API is extremely intuitive and makes it easy to perform an array of NLP tasks, such as noun phrase extraction, language translation, part-of-speech tagging, sentiment analysis, WordNet integration, and more.\n",
        "\n",
        "This library is highly recommended for anyone relatively new to developing text analysis applications, as text can be processed with just a few lines of code.\n",
        "\n",
        "**2. SpaCy**\n",
        "\n",
        "This open source Python NLP library has established itself as the go-to library for production usage, simplifying the development of applications that focus on processing significant volumes of text in a short space of time.\n",
        "\n",
        "SpaCy can be used for the preprocessing of text in deep learning environments, building systems that understand natural language and for the creation of information extraction systems.\n",
        "\n",
        "Two of the key selling points of SpaCy are that it features many pre-trained statistical models and word vectors, and has tokenization support for 49 languages. SpaCy is also preferred by many Python developers for its extremely high speeds, parsing efficiency, deep learning integration, convolutional neural network modeling, and named entity recognition capabilities.\n",
        "\n",
        "**3. Natural Language Toolkit (NLTK)**\n",
        "\n",
        "NLTK consists of a wide range of text-processing libraries and is one of the most popular Python platforms for processing human language data and text analysis. Favored by experienced NLP developers and beginners, this toolkit provides a simple introduction to programming applications that are designed for language processing purposes.\n",
        "\n",
        "Some of the key features provided by Natural Language Toolkit’s libraries include sentence detection, POS tagging, and tokenization. Tokenization, for example, is used in NLP to split paragraphs and sentences into smaller components that can be assigned specific, more understandable, meanings.\n",
        "\n",
        "NLTK’s interface is very simple, with over 50 corpora and lexical resources. Thanks to a large number of libraries made available, NLTK offers all the crucial functionality to complete almost any type of NLP task within Python.\n",
        "\n",
        "**4. Genism**\n",
        "\n",
        "Genism is a bespoke Python library that has been designed to deliver document indexing, topic modeling and retrieval solutions, using a large number of Corpora resources. Algorithms within Genism depend on memory, concerning the Corpus size. This means it can process an input that exceeds the available RAM on a system.\n",
        "\n",
        "All the popular NLP algorithms can be implemented via the library’s user-friendly interfaces, including algorithms such as Hierarchical Dirichlet Process (HDP), Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA/LSI/SVD), and Random Projections (RP).\n",
        "\n",
        "Genism’s accessibility is further enhanced by the plethora of documentation available, in addition to Jupyter Notebook tutorials. However, it should be noted that to use Genism, the Python packages SciPy and NumPy must also be installed for scientific computing functionality.\n",
        "\n",
        "**5. PyNLPl**\n",
        "\n",
        "Last on our list is PyNLPl (Pineapple), a Python library that is made of several custom Python modules designed specifically for NLP tasks. The most notable feature of PyNLPl is its comprehensive library for developing Format for Linguistic Annotation (FoLiA) XML.\n",
        "\n",
        "The platform is segmented into different packages and modules that are capable of both basic and advanced tasks, from the extraction of things like n-grams to much more complex functions. This makes it a great option for any NLP developer, regardless of their experience level."
      ],
      "metadata": {
        "id": "HgUQUut-hYLj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Explore 5 Python Text Analytics Libraries in R and write their features & applications."
      ],
      "metadata": {
        "id": "4419eWpUjVKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " R also has several powerful libraries for text analytics. Here are five popular ones with their features and applications:\n",
        "\n",
        "**tm (Text Mining Infrastructure in R):**\n",
        "\n",
        "**Features:**\n",
        "\n",
        "Corpus creation and manipulation.\n",
        "\n",
        "Text preprocessing tasks such as stemming, stop-word removal, and tokenization.\n",
        "\n",
        "Document-term matrix creation.\n",
        "\n",
        "Various text mining algorithms.\n",
        "\n",
        "Applications:\n",
        "\n",
        "Document classification.\n",
        "\n",
        "Sentiment analysis.\n",
        "\n",
        "Topic modeling.\n",
        "\n",
        "Text clustering.\n",
        "\n",
        "**quanteda:**\n",
        "\n",
        "**Features:**\n",
        "\n",
        "High-performance text analysis.\n",
        "\n",
        "Tokenization and document-feature matrix creation.\n",
        "\n",
        "Text scaling and transformation.\n",
        "\n",
        "Advanced text manipulation functions.\n",
        "\n",
        "Support for multi-language text analysis.\n",
        "\n",
        "**Applications:**\n",
        "\n",
        "Text classification.\n",
        "\n",
        "\n",
        "Topic modeling.\n",
        "\n",
        "Text clustering.\n",
        "\n",
        "Named entity recognition.\n",
        "\n",
        "NLP (Natural Language Processing) Package:\n",
        "\n",
        "**Features:**\n",
        "\n",
        "Part-of-speech tagging.\n",
        "\n",
        "Named entity recognition.\n",
        "\n",
        "Dependency parsing.\n",
        "\n",
        "Coreference resolution.\n",
        "\n",
        "**Applications:**\n",
        "\n",
        "Advanced syntactic and semantic analysis.\n",
        "\n",
        "Information extraction.\n",
        "\n",
        "Coreference resolution in documents.\n",
        "\n",
        "**tm.plugin.sentiment:**\n",
        "\n",
        "\n",
        "**Features:**\n",
        "\n",
        "Sentiment analysis on text data.\n",
        "\n",
        "Integration with external sentiment lexicons.\n",
        "\n",
        "Customizable sentiment analysis functions.\n",
        "\n",
        "**Applications:**\n",
        "\n",
        "\n",
        "Sentiment analysis in social media data.\n",
        "\n",
        "Customer review sentiment analysis.\n",
        "\n",
        "Assessing sentiment trends over time.\n",
        "\n",
        "**wordcloud:**\n",
        "\n",
        "**Features:**\n",
        "\n",
        "Creation of word clouds.\n",
        "\n",
        "Customizable word cloud appearance.\n",
        "\n",
        "Frequency-based word representation.\n",
        "\n",
        "**Applications:**\n",
        "\n",
        "Visualization of most frequent terms in a corpus.\n",
        "\n",
        "Identifying key terms in a document or set of documents.\n",
        "\n",
        "Exploring and presenting textual data in a visually appealing way.\n",
        "\n",
        "These R libraries provide a comprehensive set of tools for text analytics,\n",
        "ranging from basic text preprocessing to advanced natural language processing tasks. Depending on the specific requirements of your project, you can choose and combine these libraries to perform various text analytics tasks efficiently."
      ],
      "metadata": {
        "id": "CgHC4AhTjhXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.  Steps involved in Text Analytics"
      ],
      "metadata": {
        "id": "z62NHOB0ki4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The steps involved in analyzing an unstructured text document are :\n",
        "\n",
        "\n",
        "*   Language Identification\n",
        "*   Tokenization\n",
        "  \n",
        "*  Sentence breaking\n",
        "*  Part of Speech tagging\n",
        "*  Chunking\n",
        "*  Syntax parsing\n",
        "*  Sentence chaining"
      ],
      "metadata": {
        "id": "EaxxyZHqkr01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Benefits of Text Analytics"
      ],
      "metadata": {
        "id": "YZzKSTCSlPPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Helps in understanding emerging customer trends, product performance, and service quality.\n",
        "* Helps researchers to explore pre-existing literature and extracting what’s relevant to their study.\n",
        "* Text analytic techniques help search engines to improve their performance, thereby providing fast user experiences.\n",
        "*  Helps in making more data-driven decisions\n",
        "* Refines user content recommendation systems by categorizing related content\n",
        "* Boost Efficiency of working with Unstructured data"
      ],
      "metadata": {
        "id": "WbP_gY6KlUEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task to be performed :**"
      ],
      "metadata": {
        "id": "eNDfUZlvlxcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Explore Top-5 Text Analytics Libraries in Python (w.r.t Features & Applications)\n",
        "2. Explore Top-5 Text Analytics Libraries in R (w.r.t Features & Applications)\n",
        "3. Perform the following experiments using Python & R\n",
        "* Tokenization (Sentence & Word)\n",
        "* Frequency Distribution\n",
        "* Remove stopwords & punctuations\n",
        "* Lexicon Normalization (Stemming, Lemmatization)\n",
        "* Part of Speech tagging\n",
        "* Named Entity Recognization\n",
        "* Scrape data from a website\n",
        "4. Prepare a document with the Aim, Tasks performed, Program, Output, and Conclusion."
      ],
      "metadata": {
        "id": "ERC_ag_Yl6K7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6GwpEEVgWLk",
        "outputId": "542b7ed5-51b0-45bb-d899-ba9f4fee4ae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Text: ['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', '.']\n",
            "Filtered Tokens: ['NLTK', 'powerful', 'library', 'natural', 'language', 'processing', '.']\n",
            "Sentiment Score: {'neg': 0.0, 'neu': 0.531, 'pos': 0.469, 'compound': 0.6486}\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "text = \"NLTK is a powerful library for natural language processing.\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stopwords.words('english')]\n",
        "\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "sentiment_score = sid.polarity_scores(text)\n",
        "\n",
        "print(\"Tokenized Text:\", tokens)\n",
        "print(\"Filtered Tokens:\", filtered_tokens)\n",
        "print(\"Sentiment Score:\", sentiment_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import string\n",
        "from nltk import pos_tag, ne_chunk\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Sample text\n",
        "sample_text = \"\"\"\n",
        "Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction\n",
        "between computers and humans through natural language. It involves several tasks such as tokenization,\n",
        "lemmatization, and named entity recognition. Web scraping is a technique to extract information from websites.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenization\n",
        "sentences = sent_tokenize(sample_text)\n",
        "words = word_tokenize(sample_text)\n",
        "\n",
        "# Frequency Distribution\n",
        "fdist = FreqDist(words)\n",
        "print(\"Frequency Distribution:\")\n",
        "print(fdist.most_common(5))\n",
        "\n",
        "# Remove Stopwords & Punctuation\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_words = [word.lower() for word in words if word.lower() not in stop_words and word.lower() not in string.punctuation]\n",
        "\n",
        "# Lexicon Normalization (Stemming, Lemmatization)\n",
        "porter_stemmer = PorterStemmer()\n",
        "stemmed_words = [porter_stemmer.stem(word) for word in filtered_words]\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "# Part of Speech Tagging\n",
        "pos_tags = pos_tag(filtered_words)\n",
        "\n",
        "# Named Entity Recognition\n",
        "ner_tags = ne_chunk(pos_tags)\n",
        "\n",
        "# Web Scraping\n",
        "url = \"https://example.com\"\n",
        "response = requests.get(url)\n",
        "html_content = response.text\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "scraped_text = soup.get_text()\n",
        "\n",
        "# Print Results\n",
        "print(\"\\nTokenization (Sentence):\")\n",
        "print(sentences)\n",
        "\n",
        "print(\"\\nTokenization (Word):\")\n",
        "print(words)\n",
        "\n",
        "print(\"\\nFrequency Distribution:\")\n",
        "print(fdist.most_common(5))\n",
        "\n",
        "print(\"\\nAfter Removing Stopwords & Punctuation:\")\n",
        "print(filtered_words)\n",
        "\n",
        "print(\"\\nAfter Stemming:\")\n",
        "print(stemmed_words)\n",
        "\n",
        "print(\"\\nAfter Lemmatization:\")\n",
        "print(lemmatized_words)\n",
        "\n",
        "print(\"\\nPart of Speech Tagging:\")\n",
        "print(pos_tags)\n",
        "\n",
        "print(\"\\nNamed Entity Recognition:\")\n",
        "print(ner_tags)\n",
        "\n",
        "print(\"\\nScraped Data from Website:\")\n",
        "print(scraped_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYUsHzwLybRP",
        "outputId": "6445f079-e3e9-46cb-f4a2-db4ecfdb32b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency Distribution:\n",
            "[('.', 3), ('language', 2), ('is', 2), ('a', 2), ('and', 2)]\n",
            "\n",
            "Tokenization (Sentence):\n",
            "['\\nNatural language processing (NLP) is a field of artificial intelligence that focuses on the interaction\\nbetween computers and humans through natural language.', 'It involves several tasks such as tokenization,\\nlemmatization, and named entity recognition.', 'Web scraping is a technique to extract information from websites.']\n",
            "\n",
            "Tokenization (Word):\n",
            "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'through', 'natural', 'language', '.', 'It', 'involves', 'several', 'tasks', 'such', 'as', 'tokenization', ',', 'lemmatization', ',', 'and', 'named', 'entity', 'recognition', '.', 'Web', 'scraping', 'is', 'a', 'technique', 'to', 'extract', 'information', 'from', 'websites', '.']\n",
            "\n",
            "Frequency Distribution:\n",
            "[('.', 3), ('language', 2), ('is', 2), ('a', 2), ('and', 2)]\n",
            "\n",
            "After Removing Stopwords & Punctuation:\n",
            "['natural', 'language', 'processing', 'nlp', 'field', 'artificial', 'intelligence', 'focuses', 'interaction', 'computers', 'humans', 'natural', 'language', 'involves', 'several', 'tasks', 'tokenization', 'lemmatization', 'named', 'entity', 'recognition', 'web', 'scraping', 'technique', 'extract', 'information', 'websites']\n",
            "\n",
            "After Stemming:\n",
            "['natur', 'languag', 'process', 'nlp', 'field', 'artifici', 'intellig', 'focus', 'interact', 'comput', 'human', 'natur', 'languag', 'involv', 'sever', 'task', 'token', 'lemmat', 'name', 'entiti', 'recognit', 'web', 'scrape', 'techniqu', 'extract', 'inform', 'websit']\n",
            "\n",
            "After Lemmatization:\n",
            "['natural', 'language', 'processing', 'nlp', 'field', 'artificial', 'intelligence', 'focus', 'interaction', 'computer', 'human', 'natural', 'language', 'involves', 'several', 'task', 'tokenization', 'lemmatization', 'named', 'entity', 'recognition', 'web', 'scraping', 'technique', 'extract', 'information', 'website']\n",
            "\n",
            "Part of Speech Tagging:\n",
            "[('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('nlp', 'JJ'), ('field', 'NN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('focuses', 'VBZ'), ('interaction', 'NN'), ('computers', 'NNS'), ('humans', 'VBZ'), ('natural', 'JJ'), ('language', 'NN'), ('involves', 'VBZ'), ('several', 'JJ'), ('tasks', 'NNS'), ('tokenization', 'NN'), ('lemmatization', 'NN'), ('named', 'VBN'), ('entity', 'NN'), ('recognition', 'NN'), ('web', 'IN'), ('scraping', 'VBG'), ('technique', 'NN'), ('extract', 'JJ'), ('information', 'NN'), ('websites', 'NNS')]\n",
            "\n",
            "Named Entity Recognition:\n",
            "(S\n",
            "  natural/JJ\n",
            "  language/NN\n",
            "  processing/NN\n",
            "  nlp/JJ\n",
            "  field/NN\n",
            "  artificial/JJ\n",
            "  intelligence/NN\n",
            "  focuses/VBZ\n",
            "  interaction/NN\n",
            "  computers/NNS\n",
            "  humans/VBZ\n",
            "  natural/JJ\n",
            "  language/NN\n",
            "  involves/VBZ\n",
            "  several/JJ\n",
            "  tasks/NNS\n",
            "  tokenization/NN\n",
            "  lemmatization/NN\n",
            "  named/VBN\n",
            "  entity/NN\n",
            "  recognition/NN\n",
            "  web/IN\n",
            "  scraping/VBG\n",
            "  technique/NN\n",
            "  extract/JJ\n",
            "  information/NN\n",
            "  websites/NNS)\n",
            "\n",
            "Scraped Data from Website:\n",
            "\n",
            "\n",
            "\n",
            "Example Domain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Example Domain\n",
            "This domain is for use in illustrative examples in documents. You may use this\n",
            "    domain in literature without prior coordination or asking for permission.\n",
            "More information...\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IcJIhuHay4rO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}